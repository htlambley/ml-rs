<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source to the Rust file `src/classification/linear.rs`."><meta name="keywords" content="rust, rustlang, rust-lang"><title>linear.rs.html -- source</title><link rel="stylesheet" type="text/css" href="../../../normalize.css"><link rel="stylesheet" type="text/css" href="../../../rustdoc.css" id="mainThemeStyle"><link rel="stylesheet" type="text/css" href="../../../light.css"  id="themeStyle"><link rel="stylesheet" type="text/css" href="../../../dark.css" disabled ><link rel="stylesheet" type="text/css" href="../../../ayu.css" disabled ><script src="../../../storage.js"></script><noscript><link rel="stylesheet" href="../../../noscript.css"></noscript><link rel="icon" type="image/svg+xml" href="../../../favicon.svg">
<link rel="alternate icon" type="image/png" href="../../../favicon-16x16.png">
<link rel="alternate icon" type="image/png" href="../../../favicon-32x32.png">  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"                  integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"    integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
            ],
            macros: {
                '\\argmin': '\\mathrm{arg\\,min}'
            }
        });
    });
</script>
<style type="text/css">#crate-search{background-image:url("../../../down-arrow.svg");}</style></head><body class="rustdoc source"><!--[if lte IE 8]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="sidebar"><div class="sidebar-menu">&#9776;</div><a href='../../../ml_rs/index.html'><div class='logo-container rust-logo'><img src='../../../rust-logo.png' alt='logo'></div></a></nav><div class="theme-picker"><button id="theme-picker" aria-label="Pick another theme!"><img src="../../../brush.svg" width="18" alt="Pick another theme!"></button><div id="theme-choices"></div></div><script src="../../../theme.js"></script><nav class="sub"><form class="search-form"><div class="search-container"><div><select id="crate-search"><option value="All crates">All crates</option></select><input class="search-input" name="search" disabled autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"></div><span class="help-button">?</span>
                <a id="settings-menu" href="../../../settings.html"><img src="../../../wheel.svg" width="18" alt="Change settings"></a></div></form></nav><section id="main" class="content"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
<span id="297">297</span>
<span id="298">298</span>
<span id="299">299</span>
<span id="300">300</span>
<span id="301">301</span>
<span id="302">302</span>
<span id="303">303</span>
<span id="304">304</span>
<span id="305">305</span>
<span id="306">306</span>
<span id="307">307</span>
<span id="308">308</span>
<span id="309">309</span>
<span id="310">310</span>
<span id="311">311</span>
<span id="312">312</span>
<span id="313">313</span>
<span id="314">314</span>
<span id="315">315</span>
<span id="316">316</span>
<span id="317">317</span>
<span id="318">318</span>
<span id="319">319</span>
<span id="320">320</span>
<span id="321">321</span>
<span id="322">322</span>
<span id="323">323</span>
<span id="324">324</span>
<span id="325">325</span>
<span id="326">326</span>
<span id="327">327</span>
<span id="328">328</span>
<span id="329">329</span>
<span id="330">330</span>
<span id="331">331</span>
<span id="332">332</span>
<span id="333">333</span>
<span id="334">334</span>
<span id="335">335</span>
<span id="336">336</span>
<span id="337">337</span>
<span id="338">338</span>
<span id="339">339</span>
<span id="340">340</span>
<span id="341">341</span>
<span id="342">342</span>
<span id="343">343</span>
<span id="344">344</span>
<span id="345">345</span>
<span id="346">346</span>
<span id="347">347</span>
<span id="348">348</span>
<span id="349">349</span>
<span id="350">350</span>
<span id="351">351</span>
<span id="352">352</span>
<span id="353">353</span>
<span id="354">354</span>
<span id="355">355</span>
<span id="356">356</span>
<span id="357">357</span>
<span id="358">358</span>
<span id="359">359</span>
<span id="360">360</span>
<span id="361">361</span>
<span id="362">362</span>
<span id="363">363</span>
<span id="364">364</span>
<span id="365">365</span>
<span id="366">366</span>
<span id="367">367</span>
<span id="368">368</span>
<span id="369">369</span>
<span id="370">370</span>
<span id="371">371</span>
<span id="372">372</span>
<span id="373">373</span>
<span id="374">374</span>
<span id="375">375</span>
<span id="376">376</span>
<span id="377">377</span>
<span id="378">378</span>
<span id="379">379</span>
<span id="380">380</span>
<span id="381">381</span>
<span id="382">382</span>
<span id="383">383</span>
<span id="384">384</span>
<span id="385">385</span>
<span id="386">386</span>
<span id="387">387</span>
<span id="388">388</span>
<span id="389">389</span>
<span id="390">390</span>
<span id="391">391</span>
<span id="392">392</span>
<span id="393">393</span>
<span id="394">394</span>
<span id="395">395</span>
<span id="396">396</span>
<span id="397">397</span>
<span id="398">398</span>
<span id="399">399</span>
<span id="400">400</span>
<span id="401">401</span>
<span id="402">402</span>
<span id="403">403</span>
<span id="404">404</span>
<span id="405">405</span>
<span id="406">406</span>
<span id="407">407</span>
<span id="408">408</span>
<span id="409">409</span>
<span id="410">410</span>
<span id="411">411</span>
<span id="412">412</span>
<span id="413">413</span>
<span id="414">414</span>
<span id="415">415</span>
<span id="416">416</span>
<span id="417">417</span>
<span id="418">418</span>
<span id="419">419</span>
<span id="420">420</span>
<span id="421">421</span>
<span id="422">422</span>
<span id="423">423</span>
<span id="424">424</span>
<span id="425">425</span>
<span id="426">426</span>
<span id="427">427</span>
<span id="428">428</span>
<span id="429">429</span>
<span id="430">430</span>
<span id="431">431</span>
<span id="432">432</span>
<span id="433">433</span>
<span id="434">434</span>
<span id="435">435</span>
<span id="436">436</span>
<span id="437">437</span>
<span id="438">438</span>
<span id="439">439</span>
<span id="440">440</span>
<span id="441">441</span>
<span id="442">442</span>
<span id="443">443</span>
<span id="444">444</span>
<span id="445">445</span>
<span id="446">446</span>
<span id="447">447</span>
<span id="448">448</span>
<span id="449">449</span>
<span id="450">450</span>
<span id="451">451</span>
<span id="452">452</span>
<span id="453">453</span>
<span id="454">454</span>
<span id="455">455</span>
<span id="456">456</span>
<span id="457">457</span>
<span id="458">458</span>
<span id="459">459</span>
<span id="460">460</span>
<span id="461">461</span>
<span id="462">462</span>
<span id="463">463</span>
</pre><div class="example-wrap"><pre class="rust ">
<span class="kw">use</span> <span class="kw">super</span>::{<span class="ident">labels_binary</span>, <span class="ident">Classifier</span>, <span class="ident">ProbabilityBinaryClassifier</span>};
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">Error</span>;
<span class="kw">use</span> <span class="ident">argmin</span>::<span class="ident">core</span>;
<span class="kw">use</span> <span class="ident">argmin</span>::<span class="ident">core</span>::{<span class="ident">ArgminOp</span>, <span class="ident">Executor</span>};
<span class="kw">use</span> <span class="ident">argmin</span>::<span class="ident">solver</span>::<span class="ident">linesearch</span>::<span class="ident">MoreThuenteLineSearch</span>;
<span class="kw">use</span> <span class="ident">argmin</span>::<span class="ident">solver</span>::<span class="ident">quasinewton</span>::<span class="ident">BFGS</span>;
<span class="kw">use</span> <span class="ident">ndarray</span>::{<span class="ident">Array1</span>, <span class="ident">Array2</span>, <span class="ident">ArrayView1</span>, <span class="ident">ArrayView2</span>};
<span class="kw">use</span> <span class="ident">ndarray_linalg</span>::<span class="ident">LeastSquaresSvd</span>;
<span class="kw">use</span> <span class="ident">ndarray_rand</span>::<span class="ident">rand_distr</span>::<span class="ident">Uniform</span>;
<span class="kw">use</span> <span class="ident">ndarray_rand</span>::<span class="ident">RandomExt</span>;

<span class="doccomment">/// Represents a backend for the [`LogisticRegression`] classifier which</span>
<span class="doccomment">/// calculates appropriate weights. See [`BFGSSolver`] and [`IRLSSolver`]</span>
<span class="doccomment">/// for concrete implementations of this trait.</span>
<span class="kw">pub</span> <span class="kw">trait</span> <span class="ident">LogisticRegressionSolver</span> {
    <span class="doccomment">/// Given data matrix `x` and label array `y` (with labels converted to</span>
    <span class="doccomment">/// float), calculate suitable weights according to the chosen algorithm.</span>
    <span class="kw">fn</span> <span class="ident">fit_weights</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">x</span>: <span class="ident">ArrayView2</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>, <span class="ident">y</span>: <span class="ident">ArrayView1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>, <span class="ident">Error</span><span class="op">&gt;</span>;
}

<span class="doccomment">/// Solver for the [`LogisticRegression`] classifier implementing the</span>
<span class="doccomment">/// Broyden–Fletcher–Goldfarb–Shanno (BFGS) method.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// This solver fits the model using *maximum likelihood estimation* to find</span>
<span class="doccomment">/// the optimal weights. There is no closed form to find the maximum likelihood</span>
<span class="doccomment">/// estimator weights, so this solver obtains weights numerically using the</span>
<span class="doccomment">/// Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm. This is a quasi-Newton</span>
<span class="doccomment">/// iterative method, and further details are available in \[1\].</span>
<span class="doccomment">///</span>
<span class="doccomment">/// The optimisation algorithm is handled by the `argmin` library.</span>
<span class="doccomment">/// # References</span>
<span class="doccomment">/// \[1\] Nocedal and Wright, *Numerical Optimization*, Springer, New York,</span>
<span class="doccomment">/// NY, 2nd ed, 2006, pp. 136–144.</span>
<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Clone</span>, <span class="ident">Debug</span>)]</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">BFGSSolver</span> {
    <span class="ident">max_iter</span>: <span class="ident">u64</span>,
}

<span class="kw">impl</span> <span class="ident">Default</span> <span class="kw">for</span> <span class="ident">BFGSSolver</span> {
    <span class="kw">fn</span> <span class="ident">default</span>() <span class="op">-</span><span class="op">&gt;</span> <span class="ident">BFGSSolver</span> {
        <span class="ident">BFGSSolver</span> { <span class="ident">max_iter</span>: <span class="number">100</span> }
    }
}

<span class="kw">impl</span> <span class="ident">LogisticRegressionSolver</span> <span class="kw">for</span> <span class="ident">BFGSSolver</span> {
    <span class="kw">fn</span> <span class="ident">fit_weights</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">x</span>: <span class="ident">ArrayView2</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>, <span class="ident">y</span>: <span class="ident">ArrayView1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>, <span class="ident">Error</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">cost</span> <span class="op">=</span> <span class="ident">LogisticRegressionProblem</span> {
            <span class="ident">train_x</span>: <span class="ident">x</span>,
            <span class="ident">train_y</span>: <span class="ident">y</span>,
        };

        <span class="kw">let</span> <span class="ident">param_size</span> <span class="op">=</span> <span class="ident">x</span>.<span class="ident">ncols</span>();
        <span class="comment">// Set up a gradient descent using More–Thuente line search.</span>
        <span class="kw">let</span> <span class="ident">line_search</span> <span class="op">=</span> <span class="ident">MoreThuenteLineSearch</span>::<span class="ident">new</span>();
        <span class="kw">let</span> <span class="ident">init_hessian</span> <span class="op">=</span> <span class="ident">Array2</span>::<span class="ident">eye</span>(<span class="ident">param_size</span>);
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">BFGS</span>::<span class="ident">new</span>(<span class="ident">init_hessian</span>, <span class="ident">line_search</span>);
        <span class="comment">// Generate a random initial point in [-1, 1]^n and hope this is</span>
        <span class="comment">// reasonable.</span>
        <span class="kw">let</span> <span class="ident">x_0</span> <span class="op">=</span> <span class="ident">Array1</span>::<span class="ident">random</span>(<span class="ident">param_size</span>, <span class="ident">Uniform</span>::<span class="ident">new</span>(<span class="op">-</span><span class="number">1.0</span>, <span class="number">1.0</span>));
        <span class="kw">let</span> <span class="ident">executor</span> <span class="op">=</span> <span class="ident">Executor</span>::<span class="ident">new</span>(<span class="ident">cost</span>, <span class="ident">solver</span>, <span class="ident">x_0</span>).<span class="ident">max_iters</span>(<span class="self">self</span>.<span class="ident">max_iter</span>);
        <span class="comment">// Execute the optimiser and save the best weights found.</span>
        <span class="ident">executor</span>
            .<span class="ident">run</span>()
            .<span class="ident">map</span>(<span class="op">|</span><span class="ident">result</span><span class="op">|</span> {
                <span class="kw">let</span> <span class="ident">state</span> <span class="op">=</span> <span class="ident">result</span>.<span class="ident">state</span>;
                <span class="ident">state</span>.<span class="ident">best_param</span>.<span class="ident">to_owned</span>()
            })
            .<span class="ident">map_err</span>(<span class="op">|</span><span class="kw">_</span><span class="op">|</span> <span class="ident">Error</span>::<span class="ident">OptimiserError</span>)
    }
}

<span class="doccomment">/// Solver for the [`LogisticRegression`] classifier implementing the</span>
<span class="doccomment">/// iteratively reweighted least squares (IRLS) method.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// This solver attempts to obtain a maximum likelihood estimator for the</span>
<span class="doccomment">/// weights. In contrast to [`BFGSSolver`], this solver repeatedly solves</span>
<span class="doccomment">/// a least squares problem (which has a closed form solution) in order</span>
<span class="doccomment">/// to obtain an approximation for the weights.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Details</span>
<span class="doccomment">///</span>
<span class="doccomment">/// Let:</span>
<span class="doccomment">/// - $X$ be the data matrix with corresponding labels $y$;</span>
<span class="doccomment">/// - $w_i$ be the $i$th approxmation for the weights;</span>
<span class="doccomment">/// - $p$ the vector with entries $p_j = p(x_j; w_i)$;</span>
<span class="doccomment">/// - $W$ the diagonal matrix with entries $W_{jj} = p(x_j; w_i)(1 - p(x_j; w_i))$;</span>
<span class="doccomment">/// - $z = Xw_i  + W^{-1} (y - p)$.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// The next approximation is given by solving the *weighted least squares*</span>
<span class="doccomment">/// (WLS) problem</span>
<span class="doccomment">/// $$w_{i + 1} = \argmin_w (z - Xw)^T W (z - Xw),$$</span>
<span class="doccomment">/// where this more obviously a least squares problem by rewriting the</span>
<span class="doccomment">/// expression as</span>
<span class="doccomment">/// $$w_{i + 1} = \argmin_w \lVert W^{\frac12} (z - Xw) \rVert,$$</span>
<span class="doccomment">/// recalling that the traditional least squares problem is to minimise</span>
<span class="doccomment">/// $\lVert Ax - b \rVert$ over all $x$.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// Internally, we solve the least squares problem</span>
<span class="doccomment">/// $$w_{i + 1} = \argmin_w \lVert b - Cw \rVert,$$</span>
<span class="doccomment">/// where $b = W^{\frac12} z$ and $C = W^{\frac12} X$ using singular value</span>
<span class="doccomment">/// decompositions.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # References</span>
<span class="doccomment">/// Hastie et al, *The Elements of Statistical Learning: Data Mining,</span>
<span class="doccomment">/// Inference and Prediction*, Springer, New York, NY, 2001, 1st ed,</span>
<span class="doccomment">/// pp. 95–100.</span>
<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Clone</span>, <span class="ident">Debug</span>)]</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">IRLSSolver</span> {
    <span class="ident">max_iter</span>: <span class="ident">usize</span>,
}

<span class="kw">impl</span> <span class="ident">Default</span> <span class="kw">for</span> <span class="ident">IRLSSolver</span> {
    <span class="kw">fn</span> <span class="ident">default</span>() <span class="op">-</span><span class="op">&gt;</span> <span class="ident">IRLSSolver</span> {
        <span class="ident">IRLSSolver</span> { <span class="ident">max_iter</span>: <span class="number">10</span> }
    }
}

<span class="kw">impl</span> <span class="ident">LogisticRegressionSolver</span> <span class="kw">for</span> <span class="ident">IRLSSolver</span> {
    <span class="kw">fn</span> <span class="ident">fit_weights</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">x</span>: <span class="ident">ArrayView2</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>, <span class="ident">y</span>: <span class="ident">ArrayView1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>, <span class="ident">Error</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">weights</span>: <span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">Array1</span>::<span class="ident">zeros</span>(<span class="ident">x</span>.<span class="ident">ncols</span>());
        <span class="kw">for</span> <span class="kw">_</span> <span class="kw">in</span> <span class="number">0</span>..<span class="self">self</span>.<span class="ident">max_iter</span> {
            <span class="kw">let</span> <span class="ident">xw</span>: <span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">x</span>.<span class="ident">dot</span>(<span class="kw-2">&amp;</span><span class="ident">weights</span>);
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">p</span> <span class="op">=</span> <span class="ident">xw</span>.<span class="ident">clone</span>();
            <span class="ident">p</span>.<span class="ident">par_mapv_inplace</span>(<span class="ident">sigmoid</span>);
            <span class="comment">// Rather than directly calculating W, then inverting and taking</span>
            <span class="comment">// square roots, we note that W is a diagonal matrix with non-</span>
            <span class="comment">// negative entries, so its inverse is the reciprocal of the</span>
            <span class="comment">// non-zero diagonal elements. Its square root is simply the square</span>
            <span class="comment">// root of each diagonal element.</span>
            <span class="comment">// We first calculate the diagonal elements of W.</span>
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">w_diag</span>: <span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">p</span>.<span class="ident">clone</span>();
            <span class="ident">w_diag</span>.<span class="ident">par_mapv_inplace</span>(<span class="op">|</span><span class="ident">v</span><span class="op">|</span> {
                <span class="comment">// p * (1 - p)</span>
                <span class="ident">v</span> <span class="op">*</span> (<span class="number">1.0</span> <span class="op">-</span> <span class="ident">v</span>)
            });
            <span class="kw">let</span> <span class="ident">y_sub_p</span> <span class="op">=</span> <span class="op">-</span><span class="ident">p</span> <span class="op">+</span> <span class="kw-2">&amp;</span><span class="ident">y</span>;
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">w_inv</span> <span class="op">=</span> <span class="ident">w_diag</span>.<span class="ident">clone</span>();
            <span class="ident">w_inv</span>.<span class="ident">par_mapv_inplace</span>(<span class="op">|</span><span class="ident">v</span><span class="op">|</span> <span class="kw">if</span> <span class="ident">v</span> <span class="op">&gt;</span> <span class="number">0.0</span> { <span class="number">1.0</span> <span class="op">/</span> <span class="ident">v</span> } <span class="kw">else</span> { <span class="number">0.0</span> });
            <span class="comment">// W^{-1}(y - p)</span>
            <span class="kw">let</span> <span class="ident">prod</span>: <span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">w_inv</span>
                .<span class="ident">iter</span>()
                .<span class="ident">zip</span>(<span class="ident">y_sub_p</span>.<span class="ident">iter</span>())
                .<span class="ident">map</span>(<span class="op">|</span>(<span class="ident">x</span>, <span class="ident">y</span>)<span class="op">|</span> <span class="ident">x</span> <span class="op">*</span> <span class="ident">y</span>)
                .<span class="ident">collect</span>();
            <span class="kw">let</span> <span class="ident">z</span>: <span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">xw</span> <span class="op">+</span> <span class="ident">prod</span>;
            <span class="comment">// w_diag is now W^{1/2}.</span>
            <span class="ident">w_diag</span>.<span class="ident">par_mapv_inplace</span>(<span class="op">|</span><span class="ident">v</span><span class="op">|</span> <span class="ident">v</span>.<span class="ident">sqrt</span>());
            <span class="comment">// W^{1/2} z</span>
            <span class="kw">let</span> <span class="ident">w_sqrt_z</span> <span class="op">=</span> <span class="ident">w_diag</span>.<span class="ident">iter</span>().<span class="ident">zip</span>(<span class="ident">z</span>.<span class="ident">iter</span>()).<span class="ident">map</span>(<span class="op">|</span>(<span class="ident">a</span>, <span class="ident">b</span>)<span class="op">|</span> <span class="ident">a</span> <span class="op">*</span> <span class="ident">b</span>).<span class="ident">collect</span>();
            <span class="comment">// a will represent W^{1/2} X</span>
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">a</span> <span class="op">=</span> <span class="ident">x</span>.<span class="ident">into_owned</span>();
            <span class="ident">a</span>.<span class="ident">outer_iter_mut</span>()
                .<span class="ident">zip</span>(<span class="ident">w_diag</span>.<span class="ident">iter</span>())
                .<span class="ident">for_each</span>(<span class="op">|</span>(<span class="kw-2">mut</span> <span class="ident">row</span>, <span class="ident">w_i</span>)<span class="op">|</span> <span class="ident">row</span>.<span class="ident">iter_mut</span>().<span class="ident">for_each</span>(<span class="op">|</span><span class="ident">el</span><span class="op">|</span> <span class="kw-2">*</span><span class="ident">el</span> <span class="kw-2">*</span><span class="op">=</span> <span class="ident">w_i</span>));

            <span class="kw">let</span> <span class="ident">sol</span> <span class="op">=</span> <span class="ident">a</span>
                .<span class="ident">least_squares</span>(<span class="kw-2">&amp;</span><span class="ident">w_sqrt_z</span>)
                .<span class="ident">map_err</span>(<span class="op">|</span><span class="kw">_</span><span class="op">|</span> <span class="ident">Error</span>::<span class="ident">FittingError</span>)<span class="question-mark">?</span>
                .<span class="ident">solution</span>;
            <span class="ident">weights</span> <span class="op">=</span> <span class="ident">sol</span>;
        }
        <span class="prelude-val">Ok</span>(<span class="ident">weights</span>)
    }
}

<span class="doccomment">/// A classifier implementing the logistic regression model. Logistic</span>
<span class="doccomment">/// regression models can be used for binary classification problems and may</span>
<span class="doccomment">/// be extended through multinomial logistic regression.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// **Important**: you need to choose the solver you want to use in order</span>
<span class="doccomment">/// to use this model. See the &#39;Fitting&#39; section below for help.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Model</span>
<span class="doccomment">/// This model is appropriate if you have a collection of samples `x`</span>
<span class="doccomment">/// with labels `y` equal to `0` and `1`. In other words, this classifier</span>
<span class="doccomment">/// is suitable for **binary classification** tasks.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// Roughly speaking, the logistic regression model tries to fit a linear</span>
<span class="doccomment">/// model to predict the probability of each class. As we require probability</span>
<span class="doccomment">/// estimates to be in $[0, 1]$, the linear model is used to predict the</span>
<span class="doccomment">/// *log-odds* instead.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// ## Fitting</span>
<span class="doccomment">/// Multiple methods of fitting `LogisticRegression` are provided, so you can</span>
<span class="doccomment">/// choose whichever one you prefer or performs best on your system. The</span>
<span class="doccomment">/// current choices are:</span>
<span class="doccomment">/// - [`BFGSSolver`]</span>
<span class="doccomment">///</span>
<span class="doccomment">/// Numerically obtains the weights using a quasi-Newton iterative algorithm,</span>
<span class="doccomment">/// known as the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// - [`IRLSSolver`]</span>
<span class="doccomment">///</span>
<span class="doccomment">/// Obtains weights using a procedure known as iteratively reweighted</span>
<span class="doccomment">/// least squares, as given by Hastie et al.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// In general, [`IRLSSolver`] is slightly faster, but check this on your</span>
<span class="doccomment">/// system as it may depend on the BLAS/LAPACK you link to.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Examples</span>
<span class="doccomment">/// Fitting a logistic regression classifier and making a prediction, using the</span>
<span class="doccomment">/// BFGS solver.</span>
<span class="doccomment">/// ```no_run</span>
<span class="doccomment">/// use ndarray::array;</span>
<span class="doccomment">/// use ml_rs::classification::Classifier;</span>
<span class="doccomment">/// use ml_rs::classification::linear::{BFGSSolver, LogisticRegression};</span>
<span class="doccomment">///</span>
<span class="doccomment">/// let x = array![[1.0, 2.0, 3.0], [1.0, 4.0, 3.0]];</span>
<span class="doccomment">/// let y = array![0, 1];</span>
<span class="doccomment">///</span>
<span class="doccomment">/// let solver = BFGSSolver::default();</span>
<span class="doccomment">/// let mut clf = LogisticRegression::new(solver);</span>
<span class="doccomment">/// clf.fit(x.view(), y.view()).unwrap();</span>
<span class="doccomment">///</span>
<span class="doccomment">/// let x_test = array![[1.0, 2.0, 3.0]];</span>
<span class="doccomment">/// assert_eq!(clf.predict(x_test.view()).unwrap(), array![0]);</span>
<span class="doccomment">/// ```</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">LogisticRegression</span><span class="op">&lt;</span><span class="ident">T</span>: <span class="ident">LogisticRegressionSolver</span><span class="op">&gt;</span> {
    <span class="ident">weights</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="ident">solver</span>: <span class="ident">T</span>,
    <span class="doccomment">/// The maximum number of iterations of the BFGS algorithm to apply</span>
    <span class="doccomment">/// when fitting to the data. The default is 100. Larger values will</span>
    <span class="doccomment">/// tend to lead to better fit classifiers, but increase the time needed to</span>
    <span class="doccomment">/// train.</span>
    <span class="kw">pub</span> <span class="ident">max_iter</span>: <span class="ident">u64</span>,
}

<span class="kw">struct</span> <span class="ident">LogisticRegressionProblem</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="lifetime">&#39;b</span><span class="op">&gt;</span> {
    <span class="ident">train_x</span>: <span class="ident">ArrayView2</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="ident">f64</span><span class="op">&gt;</span>,
    <span class="ident">train_y</span>: <span class="ident">ArrayView1</span><span class="op">&lt;</span><span class="lifetime">&#39;b</span>, <span class="ident">f64</span><span class="op">&gt;</span>,
}

<span class="doccomment">/// A numerical approximation of f(x) = log(1 + exp(x)). This suffers from</span>
<span class="doccomment">/// numerical errors for moderately large x (e.g. x &gt; 700) as e^700 approaches</span>
<span class="doccomment">/// the maximum representable f64 value.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// We can try to address this problem using approximations in the limiting</span>
<span class="doccomment">/// case. The limit f(x) as x tends to infinity is x, and the limit as it</span>
<span class="doccomment">/// tends to negative infinity is e^x.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// The issue is explored in greater detail at</span>
<span class="doccomment">/// http://sachinashanbhag.blogspot.com/2014/05/numerically-approximation-of-log-1-expy.html</span>
<span class="doccomment">/// with suggested cutoff values.</span>
<span class="kw">fn</span> <span class="ident">approx_log_exp</span>(<span class="ident">x</span>: <span class="ident">f64</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">f64</span> {
    <span class="kw">if</span> <span class="ident">x</span> <span class="op">&gt;</span> <span class="number">40.</span> {
        <span class="ident">x</span>
    } <span class="kw">else</span> <span class="kw">if</span> <span class="ident">x</span> <span class="op">&lt;</span> <span class="op">-</span><span class="number">10.</span> {
        <span class="ident">x</span>.<span class="ident">exp</span>()
    } <span class="kw">else</span> {
        (<span class="number">1.</span> <span class="op">+</span> <span class="ident">x</span>.<span class="ident">exp</span>()).<span class="ident">ln</span>()
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="lifetime">&#39;b</span><span class="op">&gt;</span> <span class="ident">ArgminOp</span> <span class="kw">for</span> <span class="ident">LogisticRegressionProblem</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="lifetime">&#39;b</span><span class="op">&gt;</span> {
    <span class="kw">type</span> <span class="ident">Param</span> <span class="op">=</span> <span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>;
    <span class="kw">type</span> <span class="ident">Float</span> <span class="op">=</span> <span class="ident">f64</span>;
    <span class="kw">type</span> <span class="ident">Output</span> <span class="op">=</span> <span class="ident">f64</span>;
    <span class="kw">type</span> <span class="ident">Hessian</span> <span class="op">=</span> <span class="ident">Array2</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>;
    <span class="kw">type</span> <span class="ident">Jacobian</span> <span class="op">=</span> ();

    <span class="kw">fn</span> <span class="ident">apply</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">w</span>: <span class="kw-2">&amp;</span><span class="self">Self</span>::<span class="ident">Param</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="self">Self</span>::<span class="ident">Output</span>, <span class="ident">core</span>::<span class="ident">Error</span><span class="op">&gt;</span> {
        <span class="comment">// The log-likelihood is given by the expression</span>
        <span class="comment">// $$ \ell(w) = \langle y, Xw \rangle - \sum \log(1 + \exp((Xw)_i)). $$</span>
        <span class="comment">// We wish to maximise the log-likelihood, i.e. minimise the negative</span>
        <span class="comment">// log-likelihood.</span>

        <span class="kw">let</span> <span class="ident">xw</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">train_x</span>.<span class="ident">dot</span>(<span class="ident">w</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">log_exp_xw</span> <span class="op">=</span> <span class="ident">xw</span>.<span class="ident">clone</span>();
        <span class="ident">log_exp_xw</span>.<span class="ident">par_mapv_inplace</span>(<span class="ident">approx_log_exp</span>);
        <span class="kw">let</span> <span class="ident">log_likelihood</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">train_y</span>.<span class="ident">dot</span>(<span class="kw-2">&amp;</span><span class="ident">xw</span>) <span class="op">-</span> <span class="ident">log_exp_xw</span>.<span class="ident">sum</span>();
        <span class="prelude-val">Ok</span>(<span class="op">-</span><span class="ident">log_likelihood</span>)
    }

    <span class="kw">fn</span> <span class="ident">gradient</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">w</span>: <span class="kw-2">&amp;</span><span class="self">Self</span>::<span class="ident">Param</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="self">Self</span>::<span class="ident">Param</span>, <span class="ident">core</span>::<span class="ident">Error</span><span class="op">&gt;</span> {
        <span class="comment">// The gradient of the log-likelihood is given by the expression</span>
        <span class="comment">// $$ \nabla \ell(w) = X^T(y - \sigma(Xw)). $$</span>
        <span class="comment">// We require the gradient of the negative log-likelihood so we</span>
        <span class="comment">// negate at the end.</span>

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">xw</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">train_x</span>.<span class="ident">dot</span>(<span class="ident">w</span>);
        <span class="ident">xw</span>.<span class="ident">par_mapv_inplace</span>(<span class="ident">sigmoid</span>);
        <span class="kw">let</span> <span class="ident">inner</span> <span class="op">=</span> <span class="op">-</span><span class="ident">xw</span> <span class="op">+</span> <span class="self">self</span>.<span class="ident">train_y</span>;
        <span class="kw">let</span> <span class="ident">gradient</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">train_x</span>.<span class="ident">t</span>().<span class="ident">dot</span>(<span class="kw-2">&amp;</span><span class="ident">inner</span>);
        <span class="prelude-val">Ok</span>(<span class="op">-</span><span class="ident">gradient</span>)
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">T</span>: <span class="ident">LogisticRegressionSolver</span><span class="op">&gt;</span> <span class="ident">LogisticRegression</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span> {
    <span class="doccomment">/// Creates a new `LogisticRegression` classifier which must be fit on the</span>
    <span class="doccomment">/// data in order to find suitable weights.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">new</span>(<span class="ident">solver</span>: <span class="ident">T</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">LogisticRegression</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span> {
        <span class="ident">LogisticRegression</span> {
            <span class="ident">weights</span>: <span class="prelude-val">None</span>,
            <span class="ident">max_iter</span>: <span class="number">100</span>,
            <span class="ident">solver</span>,
        }
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">T</span>: <span class="ident">LogisticRegressionSolver</span> <span class="op">+</span> <span class="ident">Default</span><span class="op">&gt;</span> <span class="ident">Default</span> <span class="kw">for</span> <span class="ident">LogisticRegression</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span> {
    <span class="kw">fn</span> <span class="ident">default</span>() <span class="op">-</span><span class="op">&gt;</span> <span class="ident">LogisticRegression</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">T</span>::<span class="ident">default</span>();
        <span class="self">Self</span>::<span class="ident">new</span>(<span class="ident">solver</span>)
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">T</span>: <span class="ident">LogisticRegressionSolver</span><span class="op">&gt;</span> <span class="ident">Classifier</span> <span class="kw">for</span> <span class="ident">LogisticRegression</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span> {
    <span class="kw">fn</span> <span class="ident">fit</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="self">self</span>, <span class="ident">x</span>: <span class="ident">ArrayView2</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>, <span class="ident">y</span>: <span class="ident">ArrayView1</span><span class="op">&lt;</span><span class="ident">usize</span><span class="op">&gt;</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span>(), <span class="ident">Error</span><span class="op">&gt;</span> {
        <span class="kw">if</span> <span class="ident">x</span>.<span class="ident">nrows</span>() <span class="op">!</span><span class="op">=</span> <span class="ident">y</span>.<span class="ident">len</span>() <span class="op">|</span><span class="op">|</span> <span class="op">!</span><span class="ident">labels_binary</span>(<span class="ident">y</span>) {
            <span class="kw">return</span> <span class="prelude-val">Err</span>(<span class="ident">Error</span>::<span class="ident">InvalidTrainingData</span>);
        }

        <span class="comment">// Map y to an Array&lt;f64&gt; for convenience in the fitting process.</span>
        <span class="kw">let</span> <span class="ident">train_y</span> <span class="op">=</span> <span class="ident">y</span>.<span class="ident">mapv</span>(<span class="op">|</span><span class="ident">x</span><span class="op">|</span> <span class="ident">x</span> <span class="kw">as</span> <span class="ident">f64</span>);
        <span class="comment">// Pass to internal solver and obtain best weights.</span>
        <span class="kw">let</span> <span class="ident">best_param</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">solver</span>.<span class="ident">fit_weights</span>(<span class="ident">x</span>, <span class="ident">train_y</span>.<span class="ident">view</span>())<span class="question-mark">?</span>;
        <span class="self">self</span>.<span class="ident">weights</span> <span class="op">=</span> <span class="prelude-val">Some</span>(<span class="ident">best_param</span>);
        <span class="prelude-val">Ok</span>(())
    }

    <span class="kw">fn</span> <span class="ident">predict</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">x</span>: <span class="ident">ArrayView2</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">usize</span><span class="op">&gt;</span>, <span class="ident">Error</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">probabilties</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">predict_probability</span>(<span class="ident">x</span>)<span class="question-mark">?</span>;
        <span class="prelude-val">Ok</span>(<span class="ident">probabilties</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">x</span><span class="op">|</span> (<span class="ident">x</span>.<span class="ident">round</span>() <span class="kw">as</span> <span class="ident">usize</span>)).<span class="ident">collect</span>())
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">T</span>: <span class="ident">LogisticRegressionSolver</span><span class="op">&gt;</span> <span class="ident">ProbabilityBinaryClassifier</span> <span class="kw">for</span> <span class="ident">LogisticRegression</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span> {
    <span class="kw">fn</span> <span class="ident">predict_probability</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">x</span>: <span class="ident">ArrayView2</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Array1</span><span class="op">&lt;</span><span class="ident">f64</span><span class="op">&gt;</span>, <span class="ident">Error</span><span class="op">&gt;</span> {
        <span class="comment">// TODO: return an iterator so the consumer can map if necessary</span>
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">weights</span>) <span class="op">=</span> <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">weights</span> {
            <span class="comment">// Estimate the probability for each sample, and return 1 if p &gt; 0.5, 0 otherwise.</span>
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">xw</span> <span class="op">=</span> <span class="ident">x</span>.<span class="ident">dot</span>(<span class="ident">weights</span>);
            <span class="ident">xw</span>.<span class="ident">par_mapv_inplace</span>(<span class="ident">sigmoid</span>);
            <span class="prelude-val">Ok</span>(<span class="ident">xw</span>)
        } <span class="kw">else</span> {
            <span class="prelude-val">Err</span>(<span class="ident">Error</span>::<span class="ident">UseBeforeFit</span>)
        }
    }
}

<span class="doccomment">/// The sigmoid function, also called the logistic function, given by</span>
<span class="doccomment">/// f(x) = 1 / (1 + exp(-x)).</span>
<span class="kw">fn</span> <span class="ident">sigmoid</span>(<span class="ident">x</span>: <span class="ident">f64</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">f64</span> {
    <span class="number">1.0</span> <span class="op">/</span> (<span class="number">1.0</span> <span class="op">+</span> (<span class="op">-</span><span class="ident">x</span>).<span class="ident">exp</span>())
}

<span class="attribute">#[<span class="ident">cfg</span>(<span class="ident">test</span>)]</span>
<span class="kw">mod</span> <span class="ident">test</span> {
    <span class="kw">use</span> <span class="kw">super</span>::<span class="kw">super</span>::{<span class="ident">Classifier</span>, <span class="ident">Error</span>};
    <span class="kw">use</span> <span class="kw">super</span>::{<span class="ident">BFGSSolver</span>, <span class="ident">IRLSSolver</span>, <span class="ident">LogisticRegression</span>};
    <span class="kw">use</span> <span class="ident">ndarray</span>::{<span class="ident">array</span>, <span class="ident">Array1</span>, <span class="ident">Array2</span>};
    <span class="kw">use</span> <span class="ident">ndarray_rand</span>::<span class="ident">rand_distr</span>::<span class="ident">Uniform</span>;
    <span class="kw">use</span> <span class="ident">ndarray_rand</span>::<span class="ident">RandomExt</span>;

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_unfit_logistic_regression</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">BFGSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]];
        <span class="kw">match</span> <span class="ident">clf</span>.<span class="ident">predict</span>(<span class="ident">x</span>.<span class="ident">view</span>()) {
            <span class="prelude-val">Err</span>(<span class="ident">Error</span>::<span class="ident">UseBeforeFit</span>) <span class="op">=</span><span class="op">&gt;</span> (),
            <span class="kw">_</span> <span class="op">=</span><span class="op">&gt;</span> <span class="macro">panic</span><span class="macro">!</span>(<span class="string">&quot;Classifier did not return correct error&quot;</span>),
        }
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_logistic_regression_different_sizes</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">BFGSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]];
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[<span class="number">0</span>];
        <span class="kw">match</span> <span class="ident">clf</span>.<span class="ident">fit</span>(<span class="ident">x</span>.<span class="ident">view</span>(), <span class="ident">y</span>.<span class="ident">view</span>()) {
            <span class="prelude-val">Err</span>(<span class="ident">Error</span>::<span class="ident">InvalidTrainingData</span>) <span class="op">=</span><span class="op">&gt;</span> (),
            <span class="kw">_</span> <span class="op">=</span><span class="op">&gt;</span> <span class="macro">panic</span><span class="macro">!</span>(<span class="string">&quot;Classifier did not return correct error&quot;</span>),
        }
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_fit_logistic_regression</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">BFGSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">3.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">3.0</span>, <span class="number">5.0</span>]];
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>];
        <span class="ident">clf</span>.<span class="ident">fit</span>(<span class="ident">x</span>.<span class="ident">view</span>(), <span class="ident">y</span>.<span class="ident">view</span>()).<span class="ident">unwrap</span>();
        <span class="macro">assert_eq</span><span class="macro">!</span>(<span class="macro">array</span><span class="macro">!</span>[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="ident">clf</span>.<span class="ident">predict</span>(<span class="ident">x</span>.<span class="ident">view</span>()).<span class="ident">unwrap</span>());
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_fit_logistic_regression_irls</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">IRLSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">3.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">3.0</span>, <span class="number">5.0</span>]];
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>];
        <span class="ident">clf</span>.<span class="ident">fit</span>(<span class="ident">x</span>.<span class="ident">view</span>(), <span class="ident">y</span>.<span class="ident">view</span>()).<span class="ident">unwrap</span>();
        <span class="macro">assert_eq</span><span class="macro">!</span>(<span class="macro">array</span><span class="macro">!</span>[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="ident">clf</span>.<span class="ident">predict</span>(<span class="ident">x</span>.<span class="ident">view</span>()).<span class="ident">unwrap</span>());
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_logistic_regression_non_binary_labels</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">BFGSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">3.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">3.0</span>, <span class="number">5.0</span>]];
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>];
        <span class="kw">match</span> <span class="ident">clf</span>.<span class="ident">fit</span>(<span class="ident">x</span>.<span class="ident">view</span>(), <span class="ident">y</span>.<span class="ident">view</span>()) {
            <span class="prelude-val">Err</span>(<span class="ident">Error</span>::<span class="ident">InvalidTrainingData</span>) <span class="op">=</span><span class="op">&gt;</span> (),
            <span class="kw">_</span> <span class="op">=</span><span class="op">&gt;</span> <span class="macro">panic</span><span class="macro">!</span>(<span class="string">&quot;Classifier did not return correct error&quot;</span>),
        }
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_single_logistic_regression</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">BFGSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>],];
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[<span class="number">0</span>];
        <span class="ident">clf</span>.<span class="ident">fit</span>(<span class="ident">x</span>.<span class="ident">view</span>(), <span class="ident">y</span>.<span class="ident">view</span>()).<span class="ident">unwrap</span>();
        <span class="macro">assert_eq</span><span class="macro">!</span>(<span class="macro">array</span><span class="macro">!</span>[<span class="number">0</span>], <span class="ident">clf</span>.<span class="ident">predict</span>(<span class="ident">x</span>.<span class="ident">view</span>()).<span class="ident">unwrap</span>());
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_fit_logistic_regression_random</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">BFGSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">n_rows</span> <span class="op">=</span> <span class="number">2000</span>;
        <span class="kw">let</span> <span class="ident">n_features</span> <span class="op">=</span> <span class="number">5</span>;
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="ident">Array2</span>::<span class="ident">random</span>((<span class="ident">n_rows</span>, <span class="ident">n_features</span>), <span class="ident">Uniform</span>::<span class="ident">new</span>(<span class="op">-</span><span class="number">1.0</span>, <span class="number">1.0</span>));
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="ident">Array1</span>::<span class="ident">random</span>(<span class="ident">n_rows</span>, <span class="ident">Uniform</span>::<span class="ident">new</span>(<span class="number">0</span>, <span class="number">2</span>));
        <span class="ident">clf</span>.<span class="ident">fit</span>(<span class="ident">x</span>.<span class="ident">view</span>(), <span class="ident">y</span>.<span class="ident">view</span>()).<span class="ident">unwrap</span>();
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_fit_logistic_regression_random_large</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">BFGSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">n_rows</span> <span class="op">=</span> <span class="number">100000</span>;
        <span class="kw">let</span> <span class="ident">n_features</span> <span class="op">=</span> <span class="number">5</span>;
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="ident">Array2</span>::<span class="ident">random</span>((<span class="ident">n_rows</span>, <span class="ident">n_features</span>), <span class="ident">Uniform</span>::<span class="ident">new</span>(<span class="op">-</span><span class="number">1.0</span>, <span class="number">1.0</span>));
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="ident">Array1</span>::<span class="ident">random</span>(<span class="ident">n_rows</span>, <span class="ident">Uniform</span>::<span class="ident">new</span>(<span class="number">0</span>, <span class="number">2</span>));
        <span class="ident">clf</span>.<span class="ident">fit</span>(<span class="ident">x</span>.<span class="ident">view</span>(), <span class="ident">y</span>.<span class="ident">view</span>()).<span class="ident">unwrap</span>();
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_fit_irls_logistic_regression_random_large</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">IRLSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">n_rows</span> <span class="op">=</span> <span class="number">100000</span>;
        <span class="kw">let</span> <span class="ident">n_features</span> <span class="op">=</span> <span class="number">5</span>;
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="ident">Array2</span>::<span class="ident">random</span>((<span class="ident">n_rows</span>, <span class="ident">n_features</span>), <span class="ident">Uniform</span>::<span class="ident">new</span>(<span class="op">-</span><span class="number">1.0</span>, <span class="number">1.0</span>));
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="ident">Array1</span>::<span class="ident">random</span>(<span class="ident">n_rows</span>, <span class="ident">Uniform</span>::<span class="ident">new</span>(<span class="number">0</span>, <span class="number">2</span>));
        <span class="ident">clf</span>.<span class="ident">fit</span>(<span class="ident">x</span>.<span class="ident">view</span>(), <span class="ident">y</span>.<span class="ident">view</span>()).<span class="ident">unwrap</span>();
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_unfit_irls_logistic_regression</span>() {
        <span class="kw">let</span> <span class="ident">solver</span> <span class="op">=</span> <span class="ident">IRLSSolver</span>::<span class="ident">default</span>();
        <span class="kw">let</span> <span class="ident">clf</span> <span class="op">=</span> <span class="ident">LogisticRegression</span>::<span class="ident">new</span>(<span class="ident">solver</span>);
        <span class="kw">let</span> <span class="ident">x</span> <span class="op">=</span> <span class="macro">array</span><span class="macro">!</span>[[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]];
        <span class="kw">match</span> <span class="ident">clf</span>.<span class="ident">predict</span>(<span class="ident">x</span>.<span class="ident">view</span>()) {
            <span class="prelude-val">Err</span>(<span class="ident">Error</span>::<span class="ident">UseBeforeFit</span>) <span class="op">=</span><span class="op">&gt;</span> (),
            <span class="kw">_</span> <span class="op">=</span><span class="op">&gt;</span> <span class="macro">panic</span><span class="macro">!</span>(<span class="string">&quot;Classifier did not return correct error&quot;</span>),
        }
    }
}
</pre></div>
</section><section id="search" class="content hidden"></section><section class="footer"></section><script>window.rootPath = "../../../";window.currentCrate = "ml_rs";</script><script src="../../../main.js"></script><script src="../../../source-script.js"></script><script src="../../../source-files.js"></script><script defer src="../../../search-index.js"></script></body></html>