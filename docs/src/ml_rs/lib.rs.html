<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source to the Rust file `src/lib.rs`."><meta name="keywords" content="rust, rustlang, rust-lang"><title>lib.rs.html -- source</title><link rel="stylesheet" type="text/css" href="../../normalize.css"><link rel="stylesheet" type="text/css" href="../../rustdoc.css" id="mainThemeStyle"><link rel="stylesheet" type="text/css" href="../../light.css"  id="themeStyle"><link rel="stylesheet" type="text/css" href="../../dark.css" disabled ><link rel="stylesheet" type="text/css" href="../../ayu.css" disabled ><script src="../../storage.js"></script><noscript><link rel="stylesheet" href="../../noscript.css"></noscript><link rel="icon" type="image/svg+xml" href="../../favicon.svg">
<link rel="alternate icon" type="image/png" href="../../favicon-16x16.png">
<link rel="alternate icon" type="image/png" href="../../favicon-32x32.png">  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"                  integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"    integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
            ],
            macros: {
                '\\argmin': '\\mathrm{arg\\,min}'
            }
        });
    });
</script>
<style type="text/css">#crate-search{background-image:url("../../down-arrow.svg");}</style></head><body class="rustdoc source"><!--[if lte IE 8]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="sidebar"><div class="sidebar-menu">&#9776;</div><a href='../../ml_rs/index.html'><div class='logo-container rust-logo'><img src='../../rust-logo.png' alt='logo'></div></a></nav><div class="theme-picker"><button id="theme-picker" aria-label="Pick another theme!"><img src="../../brush.svg" width="18" alt="Pick another theme!"></button><div id="theme-choices"></div></div><script src="../../theme.js"></script><nav class="sub"><form class="search-form"><div class="search-container"><div><select id="crate-search"><option value="All crates">All crates</option></select><input class="search-input" name="search" disabled autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"></div><span class="help-button">?</span>
                <a id="settings-menu" href="../../settings.html"><img src="../../wheel.svg" width="18" alt="Change settings"></a></div></form></nav><section id="main" class="content"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
</pre><div class="example-wrap"><pre class="rust ">
<span class="doccomment">//! # ml-rs</span>
<span class="doccomment">//! This crate implements various machine learning algorithms in Rust,</span>
<span class="doccomment">//! built on the [`ndarray`](https://github.com/rust-ndarray/ndarray)</span>
<span class="doccomment">//! crate.</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! Currently, there is support for classification and regression models,</span>
<span class="doccomment">//! transformation (including principal component analysis) and metrics</span>
<span class="doccomment">//! to evaluate model performance.</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! # Setup</span>
<span class="doccomment">//! This crate uses BLAS/LAPACK to accelerate the linear algebra operations</span>
<span class="doccomment">//! needed. This crate is built on `ndarray-linalg`, which supports the </span>
<span class="doccomment">//! following backends:</span>
<span class="doccomment">//! - [Netlib](https://www.netlib.org/)</span>
<span class="doccomment">//! - [OpenBLAS](https://www.openblas.net/)</span>
<span class="doccomment">//! - [Intel MKL](https://software.intel.com/content/www/us/en/develop/tools/math-kernel-library.html).</span>
<span class="doccomment">//! You can choose which library you wish to link to. Roughly speaking, you </span>
<span class="doccomment">//! should expect Netlib to be slower than the other two choices, and if you</span>
<span class="doccomment">//! are deploying on Intel processors, MKL is likely to perform best (though</span>
<span class="doccomment">//! the library is not open-source).</span>
<span class="doccomment">//! </span>
<span class="doccomment">//! On Debian systems, a working configuration can be obtained by installing</span>
<span class="doccomment">//! the packages `libopenblas-base`, `libopenblas-dev` and `liblapacke-dev`,</span>
<span class="doccomment">//! and adding</span>
<span class="doccomment">//! ```toml</span>
<span class="doccomment">//! openblas-src = { version = &quot;0.7&quot;, features = [&quot;system&quot;] }</span>
<span class="doccomment">//! ```</span>
<span class="doccomment">//! to your `Cargo.toml`, which will link to the system OpenBLAS.</span>
<span class="doccomment">//! # Quick Start Guide</span>
<span class="doccomment">//! - Load data from CSV to a 2-dimensional array:</span>
<span class="doccomment">//! ```no_run</span>
<span class="doccomment">//! use ml_rs::preprocessing::CsvReader;</span>
<span class="doccomment">//! use ndarray::Array2;</span>
<span class="doccomment">//! use std::fs::File;</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! let csv_file = File::open(&quot;data.csv&quot;).unwrap();</span>
<span class="doccomment">//! let reader = CsvReader::new(&amp;csv_file);</span>
<span class="doccomment">//! // Pass the number of rows and columns expected</span>
<span class="doccomment">//! let n_columns = 5;</span>
<span class="doccomment">//! let n_rows = 1000;</span>
<span class="doccomment">//! let data: Array2&lt;f64&gt; = reader.read(n_rows, n_columns);</span>
<span class="doccomment">//! ```</span>
<span class="doccomment">//! - Separate into data matrix and target vector:</span>
<span class="doccomment">//! ```</span>
<span class="doccomment">//! # use ndarray::array;</span>
<span class="doccomment">//! # let data = array![[-1.0, 1.0, 1.0, -1.0, 3.0], [2.0, 4.0, 5.0, 1.0, 6.0]];</span>
<span class="doccomment">//! use ndarray::{Axis, s};</span>
<span class="doccomment">//! // Choose the last column of the `data` array to be the target</span>
<span class="doccomment">//! let feature_col_index = 4;</span>
<span class="doccomment">//! let x = data.slice(s![.., 0..feature_col_index]);</span>
<span class="doccomment">//! let y = data.index_axis(Axis(1), feature_col_index);</span>
<span class="doccomment">//! ```</span>
<span class="doccomment">//! - Perform regression:</span>
<span class="doccomment">//! ```no_run</span>
<span class="doccomment">//! # use ndarray::array;</span>
<span class="doccomment">//! # let _x = array![[-1.0, 1.0, 1.0, -1.0], [2.0, 4.0, 5.0, 1.0]];</span>
<span class="doccomment">//! # let _y = array![3.0, 6.0];</span>
<span class="doccomment">//! # let x = _x.view();</span>
<span class="doccomment">//! # let y = _y.view();</span>
<span class="doccomment">//! use ml_rs::regression::linear::LinearRegression;</span>
<span class="doccomment">//! use ml_rs::regression::Regressor;</span>
<span class="doccomment">//! let mut lm = LinearRegression::new();</span>
<span class="doccomment">//! // Fit a linear regression model to the data, unwrapping to check for errors</span>
<span class="doccomment">//! lm.fit(x, y).unwrap();</span>
<span class="doccomment">//! // Get the predicted values for `x` given by the regression model</span>
<span class="doccomment">//! let y_pred = lm.predict(x);</span>
<span class="doccomment">//! ```</span>
<span class="doccomment">//! - Measure the performance of a model:</span>
<span class="doccomment">//! ```</span>
<span class="doccomment">//! # let _y = array![3.0, 6.0];</span>
<span class="doccomment">//! # let y = _y.view();</span>
<span class="doccomment">//! # let y_pred = array![3.0, 6.0];</span>
<span class="doccomment">//! use ml_rs::metrics::accuracy_score;</span>
<span class="doccomment">//! // We own `y_pred`, so we need to return a view, which means we</span>
<span class="doccomment">//! // don&#39;t consume it when calculating accuracy.</span>
<span class="doccomment">//! let train_accuracy = accuracy_score(y, y_pred.view());</span>
<span class="doccomment">//! println!(&quot;Training set accuracy: {}%&quot;, train_accuracy * 100.0);</span>
<span class="doccomment">//! ```</span>
<span class="doccomment">//! Classification works very similarly to regression: for an example,</span>
<span class="doccomment">//! see the [`classification`] module.</span>

<span class="attribute">#[<span class="ident">warn</span>(<span class="ident">missing_docs</span>)]</span>

<span class="doccomment">/// A variety of supervised classification models to use with numeric data.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// Classification tasks aim to construct a model</span>
<span class="doccomment">/// $h \colon \mathcal{X} \to \\{0, 1, \dots, n - 1\\}$ to distinguish between</span>
<span class="doccomment">/// $n$ classes of data from the data space $\mathcal{X}$, which is typically</span>
<span class="doccomment">/// $\mathbb{R}^m$. Classification is a *supervised learning* task which</span>
<span class="doccomment">/// requires some pre-labelled training data sampled independently from</span>
<span class="doccomment">/// the data distribution.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Context</span>
<span class="doccomment">/// When designing a classifier, we start with some training data</span>
<span class="doccomment">/// $(X_1, Y_1), \dots, (X_p, Y_p)$, and choose a *model*, which determines the</span>
<span class="doccomment">/// collection $\mathcal{H}$ of classifiers that we want to choose from.</span>
<span class="doccomment">/// Generally, we then proceed by trying to find the classifier in</span>
<span class="doccomment">/// $\mathcal{H}$ that minimises the error over the training data, using some</span>
<span class="doccomment">/// suitable algorithm. We then evaluate the performance of the model on some</span>
<span class="doccomment">/// new data sampled from the data distribution in order to estimate the</span>
<span class="doccomment">/// generalisation error.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// This library supports the procedure by providing several models, listed</span>
<span class="doccomment">/// below, which can be fit on data, and some tools in the `metrics` module</span>
<span class="doccomment">/// to evaluate the performance of models on new data. The steps to take are:</span>
<span class="doccomment">/// - Load the dataset into memory as a *data matrix* $X$ and an array of</span>
<span class="doccomment">///   *labels* $y$.</span>
<span class="doccomment">/// - Choose a suitable model and fit it (see the `Classifier` trait) on</span>
<span class="doccomment">///   *training data*.</span>
<span class="doccomment">/// - Use a scoring function from `metrics` such as the accuracy score to</span>
<span class="doccomment">///   evaluate the performance on some *test data* that is distinct from</span>
<span class="doccomment">///   the training data.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// An overview of model selection can be found in \[1\]. Bibliographic</span>
<span class="doccomment">/// references to the models provided by the library are provided where</span>
<span class="doccomment">/// appropriate in the documentation of the respective classifier.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// ## Statistical Learning Theory</span>
<span class="doccomment">/// This section can freely be omitted, but provides interesting mathematical</span>
<span class="doccomment">/// formalism which explains why the procedures we use are justified.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// We begin with a data space $\mathcal{X}$, and the corresponding *target</span>
<span class="doccomment">/// space* $\mathcal{Y} = \\{0, 1, \dots, n - 1\\}$. We assume that the data</span>
<span class="doccomment">/// pairs $(x, y) \in \mathcal{X} \times \mathcal{Y}$ emerge frome some</span>
<span class="doccomment">/// probability disribution $\mathcal{P}$ on $\mathcal{X} \times \mathcal{Y}$,</span>
<span class="doccomment">/// and that the training data are *independent and identically distributed*</span>
<span class="doccomment">/// (i.i.d.) samples from $\mathcal{P}$. The goal is to learn the label of</span>
<span class="doccomment">/// any sample $x \in \mathcal{X}$: in other words, we would like to know</span>
<span class="doccomment">/// the conditional probability $\mathbb{P} [ Y = y \mid X \in A]$ for any</span>
<span class="doccomment">/// subset $A$ of $\mathcal{X}$. We would expect that if the training data</span>
<span class="doccomment">/// are i.i.d., then we should be able to make a good estimation of the</span>
<span class="doccomment">/// conditional probability if we have sufficient data.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// We choose a function to measure the *risk*, $R(h)$, that a given classifier</span>
<span class="doccomment">/// $h$ makes an error. This is taken over the entire distribution with respect</span>
<span class="doccomment">/// to some *loss function*, so if $(X, Y)$ are sampled from $\mathcal{P}$,</span>
<span class="doccomment">/// then</span>
<span class="doccomment">/// $$ R(h) = \mathbb{E} [ L(h(X), Y) ]. $$</span>
<span class="doccomment">/// We can estimate the risk over the entire distribution by the *empirical</span>
<span class="doccomment">/// risk*, given some training data $T = \\{ (X_1, Y_1), \dots, (X_p, Y_p) \\}$:</span>
<span class="doccomment">/// $$R_\mathrm{E}(h; T) = \frac1p \sum_{i = 1}^p L(h(X_i), Y_i).$$</span>
<span class="doccomment">/// Provided that the training data are indeed i.i.d., the expected value of</span>
<span class="doccomment">/// the empirical risk is the (generalisation) risk $R(h)$, so the empirical</span>
<span class="doccomment">/// risk serves as an estimate of generalisation error. If the data are not</span>
<span class="doccomment">/// i.i.d. then the empirical risk may not be a good estimate of the</span>
<span class="doccomment">/// true risk, leading to poor performance on unseen data.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// Choosing a model amounts to selecting a *hypothesis class* $\mathcal{H}$,</span>
<span class="doccomment">/// which is a collection of functions which we consider as candidates. The</span>
<span class="doccomment">/// *empirical risk minimisation* problem is to find the classifier in</span>
<span class="doccomment">/// $\mathcal{H}$ that best fits the data:</span>
<span class="doccomment">/// $$ \argmin_{h \in \mathcal{H}} R_\mathrm{E}(h; T). $$</span>
<span class="doccomment">///</span>
<span class="doccomment">/// The above is a standard characterisation of statistical learning theory.</span>
<span class="doccomment">/// A much broader book on the topic is \[2\].</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Models</span>
<span class="doccomment">/// Currently, this library supports the following models.</span>
<span class="doccomment">/// ## Trivial Models</span>
<span class="doccomment">/// - [`classification::TrivialClassifier`]</span>
<span class="doccomment">/// - [`classification::MajorityClassifier`].   </span>
<span class="doccomment">/// ## Logistic Regression (in `linear`)</span>
<span class="doccomment">/// These models currently only support binary classification. They are</span>
<span class="doccomment">/// appropriate where a linear function of the features would be a good</span>
<span class="doccomment">/// predictor of the probability of lying in the positive class.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// - [`classification::linear::LogisticRegression`].</span>
<span class="doccomment">///</span>
<span class="doccomment">/// Multiple solvers are provided, as can be viewed on the main documentation</span>
<span class="doccomment">/// page; it is advisable to try all the options to see which perform best.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Examples</span>
<span class="doccomment">/// For examples, see the classifiers above, which are provided with</span>
<span class="doccomment">/// usage examples.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # References</span>
<span class="doccomment">/// \[1\] Hastie et al, *The Elements of Statistical Learning: Data Mining,</span>
<span class="doccomment">/// Inference and Prediction*, Springer, New York, NY, 2001, 1st ed,</span>
<span class="doccomment">/// ch. 7.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// \[2\] Vapnik, *The Nature of Statistical Learning Theory*, Springer, New</span>
<span class="doccomment">/// York, NY, 1999, 1st ed.</span>
<span class="kw">pub</span> <span class="kw">mod</span> <span class="ident">classification</span>;

<span class="doccomment">/// A collection of metrics to measure the performance of classification</span>
<span class="doccomment">/// and regression models.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// After developing a model, these metrics allow you to evaluate the</span>
<span class="doccomment">/// performance and decide whether to refine, reject or accept the model.</span>
<span class="kw">pub</span> <span class="kw">mod</span> <span class="ident">metrics</span>;
<span class="doccomment">/// Utilities including loading data from CSV files to arrays to input into</span>
<span class="doccomment">/// models.</span>
<span class="kw">pub</span> <span class="kw">mod</span> <span class="ident">preprocessing</span>;
<span class="doccomment">/// A collection of supervised regression models to predict continuous</span>
<span class="doccomment">/// variables from data.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// Regression tasks look to construct a regressor</span>
<span class="doccomment">/// $h \colon \mathcal{X} \to \mathcal{Y}$, where instead of $\mathcal{Y}$</span>
<span class="doccomment">/// being a finite set as in classification, we have $\mathcal{Y}$ being a</span>
<span class="doccomment">/// continuum, e.g. an interval $[a, b]$.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Context</span>
<span class="doccomment">/// The context is largely the same as in the classification case (and reading</span>
<span class="doccomment">/// the [`classification`] module documentation should prove helpful). The</span>
<span class="doccomment">/// main change is the the space $\mathcal{Y}$ which is now continuous, so some</span>
<span class="doccomment">/// classification models do not have corresponding regression models, whereas</span>
<span class="doccomment">/// others (such as least squares regression) find applications in both</span>
<span class="doccomment">/// classification and regression.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Models</span>
<span class="doccomment">/// Currently, the following regressors are supported.</span>
<span class="doccomment">/// ## Linear Regression</span>
<span class="doccomment">/// These models are appropriate when the target $Y$ is believed to be some</span>
<span class="doccomment">/// linear function $f(X)$ of the feature vector $X$.</span>
<span class="doccomment">/// - [`regression::linear::LinearRegression`].</span>
<span class="kw">pub</span> <span class="kw">mod</span> <span class="ident">regression</span>;
<span class="doccomment">/// Procedures to perform scaling, dimensionality reduction and other</span>
<span class="doccomment">/// transformations on the data before input into a model.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Details</span>
<span class="doccomment">/// Transformers in this library can be regarded as various maps on data</span>
<span class="doccomment">/// matrices in $\mathcal{X}^n$. A transformer $T$ is generally a function</span>
<span class="doccomment">/// $$T \colon \mathcal{X}^n \to \mathcal{Z}^n,$$</span>
<span class="doccomment">/// where $\mathcal{Z}$ is some other data space that we transform to.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// A *dimensionality reduction* transformer is a transformation where</span>
<span class="doccomment">/// $\mathcal{X} = \mathbb{R}^m$, and $\mathcal{Z} = \mathbb{R}^p$, with</span>
<span class="doccomment">/// $p &lt; m$. We could perform trivial dimensionality reduction by deleting</span>
<span class="doccomment">/// certain components of each sample vector, or we could perform a more</span>
<span class="doccomment">/// nuanced transformation. Among the most famous dimensionality reduction</span>
<span class="doccomment">/// procedures is *principal component analysis* as proposed by Pearson (1901).</span>
<span class="doccomment">/// This is implemented in ml-rs as [`transformation::pca::PrincipalComponentAnalysis`].</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # Prior Art</span>
<span class="doccomment">/// The idea of a common API for classification, regression and transformation</span>
<span class="doccomment">/// is used to great success in scikit-learn, a Python machine learning library.</span>
<span class="doccomment">///</span>
<span class="doccomment">/// # References</span>
<span class="doccomment">/// Pedregosa et al, *Scikit-learn: Machine Learning in Python*, J. Machine</span>
<span class="doccomment">/// Learning Research 12, 2011, pp. 2825–2830.</span>
<span class="kw">pub</span> <span class="kw">mod</span> <span class="ident">transformation</span>;

<span class="kw">use</span> <span class="ident">thiserror</span>::<span class="ident">Error</span>;

<span class="doccomment">/// The main error type which represents an error in a model or transformer.</span>
<span class="doccomment">/// As there are many commonalities between classifiers, regressors and</span>
<span class="doccomment">/// transformers, this general error is returned during the use of any of</span>
<span class="doccomment">/// these objects.</span>
<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Clone</span>, <span class="ident">Debug</span>, <span class="ident">Error</span>)]</span>
<span class="kw">pub</span> <span class="kw">enum</span> <span class="ident">Error</span> {
    <span class="doccomment">/// A model or transformer that required fitting was not fit before trying</span>
    <span class="doccomment">/// to use (e.g. calling `predict()` before `fit()`).</span>
    <span class="attribute">#[<span class="ident">error</span>(<span class="string">&quot;attempted to use before calling `fit()`: try fitting with appropriate training data before usage&quot;</span>)]</span>
    <span class="ident">UseBeforeFit</span>,
    <span class="doccomment">/// The training data provided was invalid in some sense. Check the</span>
    <span class="doccomment">/// assumptions for the model used. Common problems are:</span>
    <span class="doccomment">/// - the data matrix `x` and the label array `y` are different lengths</span>
    <span class="doccomment">/// - the data matrix `x` or label array `y` are empty.</span>
    <span class="attribute">#[<span class="ident">error</span>(<span class="string">&quot;provided training data was invalid&quot;</span>)]</span>
    <span class="ident">InvalidTrainingData</span>,
    <span class="doccomment">/// The model being used requires an optimisation problem to be solved,</span>
    <span class="doccomment">/// but when passing the problem to `argmin`, an error occurred.</span>
    <span class="doccomment">/// This is most likely an internal error that is not caused by the</span>
    <span class="doccomment">/// user input: file an issue if this occurs without obvious cause.</span>
    <span class="attribute">#[<span class="ident">error</span>(<span class="string">&quot;attempted to solve optimisation problem, but the optimiser encountered an error&quot;</span>)]</span>
    <span class="ident">OptimiserError</span>,
    <span class="doccomment">/// The model had an error during the fitting process. Common problems are:</span>
    <span class="doccomment">/// - the fitting process involved a call to LAPACK which failed.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This may be an internal error, or a problem on your system.</span>
    <span class="attribute">#[<span class="ident">error</span>(<span class="string">&quot;an error occurred during the fitting process&quot;</span>)]</span>
    <span class="ident">FittingError</span>,
}
</pre></div>
</section><section id="search" class="content hidden"></section><section class="footer"></section><script>window.rootPath = "../../";window.currentCrate = "ml_rs";</script><script src="../../main.js"></script><script src="../../source-script.js"></script><script src="../../source-files.js"></script><script defer src="../../search-index.js"></script></body></html>